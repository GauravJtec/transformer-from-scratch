{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca34d79e-17bb-4f33-a5dc-a15b84669270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10791217-5d41-44ff-9d03-42331a361075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q,k,v,mask=None):\n",
    "    #q,k,v=30*8*200*64\n",
    "    d_k=q.size()[-1]#64\n",
    "    scaled=torch.matmul(q,k.transpose(-1,-2))/math.sqrt(d_k)#30*8*200*200\n",
    "    if mask is not None:\n",
    "        scaled+=mask#30*8*200*200\n",
    "    attention=F.softmax(scaled,dim=-1)#30*8*200*200\n",
    "    values=torch.matmul(attention,v)#30*8*200*64\n",
    "    return values,attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model#512\n",
    "        self.num_heads=num_heads#8\n",
    "        self.head_dim=d_model//num_heads#64\n",
    "        self.qkv_layer=nn.Linear(d_model,3*d_model)#512,1536\n",
    "        self.linear_layer=nn.Linear(d_model,d_model)#512,512\n",
    "\n",
    "    def forward(self,x,mask=None):\n",
    "        batch_size,sequence_length,d_model=x.size()#30*200*512\n",
    "        qkv=self.qkv_layer(x)#30*200*1536\n",
    "        qkv=qkv.reshape(batch_size,sequence_length,self.num_heads,3*self.head_dim)#30*200*8*192\n",
    "        qkv=qkv.permute(0,2,1,3)#30*8*200*192\n",
    "        q,k,v=qkv.chunk(3,dim=-1)#each are 30*8*200*64\n",
    "        values,attention=scaled_dot_product(q,k,v,mask)#attention=30*8*200*200 , values=30*8*200*64\n",
    "        values=values.reshape(batch_size,sequence_length,self.num_heads*self.head_dim)+30*200*512\n",
    "        out=self.linear_layer(values)\n",
    "        return out\n",
    "\n",
    "class LayerNormalization(nn.Module)\n",
    "    def __init__(self,parameters_shape,eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape#512\n",
    "        self.eps=eps#\n",
    "        self.gamma=nn.Parameter(torch.ones(parameters_shape))#512\n",
    "        self.beta=nn.Parameter(torch.zeros(parameters_shape))#512\n",
    "\n",
    "    def forward(self,inputs):#30*200*512\n",
    "        dims=[-(i+1) for i in range(len(self.parameters_shape))]#-1\n",
    "        mean=inputs.mean(dims=dims,keepdim=True)#300*200*1\n",
    "        var=((input-mean)**2).mean(dim=dims,keepdims=True)#300*200*1\n",
    "        std=(var+self.eps).sqrt()#300*200*1\n",
    "        y=(inputs-mean)/std#300*200*512\n",
    "        out=self.gamma*y+self.beta\n",
    "        return out\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_model,hidden,drop_prob=.1):\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.linear1=nn.Linear(d_model,hidden)#512*2048\n",
    "        self.linear2=nn.Linear(hidden,d_model)#2048*512\n",
    "        self.relu=nn.Relu()\n",
    "        self.dropout=nn.Dropout(p=drop_prob)\n",
    "\n",
    "\n",
    "    def forward(self,x):#30*200*512\n",
    "        x=self.linear1(x)#30*200*2048\n",
    "        x=self.relu(x)#30*200*2048\n",
    "        x=self.dropout(x)#30*200*2048\n",
    "        x=self.linear2(x)#30*200*512\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.attention=MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
    "        self.norm1=LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1=nn.Dropout(p=drop_prob)\n",
    "        self.ffn=PositionwiseFeedForward(d_model=d_model,hidden=ffn_hidden,drop_prob=drop_prob)\n",
    "        self.norm2=LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2=nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self,x):\n",
    "        residual_x=x#300*200*512\n",
    "        x=self.attention(x,mask=None)#300*200*512\n",
    "        x=self.dropout1(x)#300*200*512\n",
    "        x=self.norm1(x+residual_x)#300*200*512\n",
    "        residual_x=x#300*200*512\n",
    "        x=self.ffn(x)#300*200*512\n",
    "        x=self.dropout2(x)#300*200*512\n",
    "        x=self.norm2(x+residual_x)#300*200*512\n",
    "        return x\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,num_heads,drop_prob,num_layers):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(*[EncoderLayer(d_model,ffn_hidden,num_heads,drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.layers(x)\n",
    "        return x       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ade063-a956-438d-8a45-ef91a34637ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=512\n",
    "num_heads=8\n",
    "drop_prob=.1\n",
    "batch_size=30\n",
    "max_sequence_length=200\n",
    "ffn_hidden=2048\n",
    "num_layer=5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa7e62f-146d-4068-8980-8fecf45ac0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn((batch_size,max_sequence_length,d_model))\n",
    "out=encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3dd1bd-ab74-4017-aec4-c54d2bc4cec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689ac0d-08ec-4d3b-9b3e-bd6b968e5c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc75a7-f3cd-4304-bc52-b25a3402424f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e567f-bcd4-401e-815d-e3b436824dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c2317-7594-4895-a13a-f38dd9b80292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31713119-44d4-4dc6-8f03-ab902b97eba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a457b52-413f-49f0-9aa9-5dd5aa30419d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546cab4-9819-40b2-885d-c905ead8dc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32336ec-5436-49ab-92c6-4a53119428c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb088d2-634f-45a4-9865-4d28c7ef4953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5231095-ecdf-458a-ac56-64941cca24d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d704e1c-d620-45fa-88f6-8c44758deeee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8450cc0-5dfb-4896-8579-9c44a4142bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266ab690-a7bf-4898-8847-83becccc05e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53848ca3-1ebb-450d-84df-eb9925caf4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3ead6-2e71-4f6e-b427-425b04aed9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68932229-23a5-41fc-8e03-5716d93f0506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c4ab18-827e-4ea8-afe2-14b1a863c000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9e370-ee3f-48a6-b96c-13a7e6380c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014442e2-9d24-4578-82dd-89fbe1be0154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0c6360-f10e-41b9-a5f7-36f9c8cc3be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34837e36-0dec-4668-acd9-627aa93c8b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1860b-4924-415e-a87e-c00bfa21c552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f3d46-cc33-4e4a-be41-5ca3066937ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60e4cd-70d8-4561-ad04-e3e3c17d6e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882737a5-e21e-4566-857a-31279a1d0d84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
