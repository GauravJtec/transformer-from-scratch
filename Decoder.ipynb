{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa6534b-be90-42ae-b19f-f69273caaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product(q,k,v,mask=None):\n",
    "    #q,k,v=30*8*200*64\n",
    "    d_k=q.size()[-1]#64\n",
    "    scaled=torch.matmul(q,k.transpose(-1,-2))/math.sqrt(d_k)#30*8*200*200\n",
    "    if mask is not None:\n",
    "        scaled+=mask#30*8*200*200\n",
    "    attention=F.softmax(scaled,dim=-1)#30*8*200*200\n",
    "    values=torch.matmul(attention,v)#30*8*200*64\n",
    "    return values,attention\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self,parameters_shape,eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape#512\n",
    "        self.eps=eps#\n",
    "        self.gamma=nn.Parameter(torch.ones(parameters_shape))#512\n",
    "        self.beta=nn.Parameter(torch.zeros(parameters_shape))#512\n",
    "\n",
    "    def forward(self,inputs):#30*200*512\n",
    "        dims=[-(i+1) for i in range(len(self.parameters_shape))]#-1\n",
    "        mean=inputs.mean(dims=dims,keepdim=True)#300*200*1\n",
    "        var=((input-mean)**2).mean(dim=dims,keepdims=True)#300*200*1\n",
    "        std=(var+self.eps).sqrt()#300*200*1\n",
    "        y=(inputs-mean)/std#300*200*512\n",
    "        out=self.gamma*y+self.beta\n",
    "        return out\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_model,hidden,drop_prob=.1):\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.linear1=nn.Linear(d_model,hidden)#512*2048\n",
    "        self.linear2=nn.Linear(hidden,d_model)#2048*512\n",
    "        self.relu=nn.Relu()\n",
    "        self.dropout=nn.Dropout(p=drop_prob)\n",
    "\n",
    "\n",
    "    def forward(self,x):#30*200*512\n",
    "        x=self.linear1(x)#30*200*2048\n",
    "        x=self.relu(x)#30*200*2048\n",
    "        x=self.dropout(x)#30*200*2048\n",
    "        x=self.linear2(x)#30*200*512\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model#512\n",
    "        self.num_heads=num_heads#8\n",
    "        self.head_dim=d_model//num_heads#64\n",
    "        self.qkv_layer=nn.Linear(d_model,3*d_model)#512,1536\n",
    "        self.linear_layer=nn.Linear(d_model,d_model)#512,512\n",
    "\n",
    "    def forward(self,x,mask=None):\n",
    "        batch_size,sequence_length,d_model=x.size()#30*200*512\n",
    "        qkv=self.qkv_layer(x)#30*200*1536\n",
    "        qkv=qkv.reshape(batch_size,sequence_length,self.num_heads,3*self.head_dim)#30*200*8*192\n",
    "        qkv=qkv.permute(0,2,1,3)#30*8*200*192\n",
    "        q,k,v=qkv.chunk(3,dim=-1)#each are 30*8*200*64\n",
    "        values,attention=scaled_dot_product(q,k,v,mask)#attention=30*8*200*200 , values=30*8*200*64\n",
    "        values=values.reshape(batch_size,sequence_length,self.num_heads*self.head_dim)+30*200*512\n",
    "        out=self.linear_layer(values)\n",
    "        return out\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model#512\n",
    "        self.num_heads=num_heads#8\n",
    "        self.head_dim=d_model//num_heads#64\n",
    "        self.kv_layer=nn.Linear(d_model,2*d_model)#1024\n",
    "        self.q_layer=nn.Linear(d_model,d_model)\n",
    "        self.linear_layer=nn.Linear(d_model,d_model)#512,512\n",
    "\n",
    "    def forward(self,x,mask=None):\n",
    "        batch_size,sequence_length,d_model=x.size()#30*200*512\n",
    "        kv=self.kv_layer(x)#30*200*1024\n",
    "        q=self.q_layer(y)#30*200*512\n",
    "        kv=kv.reshape(batch_size,sequence_length,self.num_heads,2*self.head_dim)#30*200*8*128\n",
    "        q=q.reshape(batch_size,sequence_length,self.num_heads,self.head_dim)#30*200*8*64\n",
    "        kv=kv.permute(0,2,1,3)#30*8*200*128\n",
    "        q=q.permute(0,2,1,3)#30*8*200*64\n",
    "        k,v=kv.chunk(2,dim=-1)#30*8*200*64 each\n",
    "        values,attention=scaled_dot_product(q,k,v,mask)#30*8*200*64\n",
    "        values=values.reshape(batch_size,sequence_length,self.num_heads*self.head_dim)\n",
    "        out=self.linear_layer(values)#30*200*512\n",
    "        return out\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.self_attention=MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
    "        self.norm1=LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1=nn.Dropout(p=drop_prob)\n",
    "        self.encoder_decoder__attention=MultiHeadCrossAttention(d_model=d_model,num_heads=num_heads)\n",
    "        self.norm2=LayerNormalization(parameters_shape=[d_model])\n",
    "        self.ffn=PositionwiseFeedForward(d_model=d_model,hidden=ffn_hidden,drop_prob=drop_prob)\n",
    "        self.norm3=LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout3=nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self,x,y,decoder_mask):\n",
    "        _y=y\n",
    "        y=self.self_attention(y,mask=decoder_mask)\n",
    "        y=self.dropout1(y)\n",
    "        y=self.norm1(y+_y)\n",
    "\n",
    "         _y=y\n",
    "        y=self.self.encoder_decoder_attentiony(x,y,mask=None)#30*200*512\n",
    "        y=self.dropout2(y)#30*200*512\n",
    "        y=self.norm2(y+_y)#30*200*512\n",
    "\n",
    "\n",
    "         _y=y\n",
    "        y=self.self.ffn(y)#30*200*512\n",
    "        y=self.dropout3(y)#30*200*512\n",
    "        y=self.norm3(y+_y)#30*200*512\n",
    "        return y\n",
    "\n",
    "    def forward(self,x):\n",
    "        residual_x=x#300*200*512\n",
    "        x=self.attention(x,mask=None)#300*200*512\n",
    "        x=self.dropout1(x)#300*200*512\n",
    "        x=self.norm1(x+residual_x)#300*200*512\n",
    "        residual_x=x#300*200*512\n",
    "        x=self.ffn(x)#300*200*512\n",
    "        x=self.dropout2(x)#300*200*512\n",
    "        x=self.norm2(x+residual_x)#300*200*512\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214da67-1bac-49d2-927a-103af58c6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDecode(nn.Sequential):\n",
    "      def forward(self,*inputs):\n",
    "          x,y,mask=inputs\n",
    "          for module in self._module.values():\n",
    "              y=module(x,y,mask)\n",
    "          return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10e8eb-e52f-4d21-904b-325ca6e7d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,num_heads,drop_prob,num_layers=1):\n",
    "        super().__init__()\n",
    "        self.layers=SequentialDecoder(*[DecoderLayer(d_model,ffn_hidden,num_heads,drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self,x,y,mask):\n",
    "        #x=30*200*512 and y=30*200*512,mask=200*200\n",
    "        y=self.layers(x,y,mask)\n",
    "        return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc19a8-62eb-4be4-916c-95c6d020201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=512\n",
    "num_heads=8\n",
    "drop_prob=.1\n",
    "batch_size=30\n",
    "max_sequence_length=200\n",
    "ffn_hidden=2048\n",
    "num_layer=5\n",
    "\n",
    "x=torch.randn((batch_size,max_sequence_length,d_model))# english sentence positional encoding\n",
    "y=torch.randn((batch_size,max_sequence_length,d_model))# hindi sentence positional encoding\n",
    "mask=torch.full([max_sequence_length,max_sequence_length],float('-inf'))\n",
    "mask=torch.triu(mask,diagonal=1)\n",
    "decoder=Decoder(d_model,ffn_hidden,num_heads,drop_prob,num_layers)\n",
    "out=decoder(x,y,mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
